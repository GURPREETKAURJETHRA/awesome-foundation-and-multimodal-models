title, date, paper, code
"Kosmos-2: Grounding Multimodal Large Language Models to the World",26-07-2023,2306.14824,https://github.com/microsoft/unilm
"LLaVA: Large Language and Vision Assistant",17-04-2023,2304.08485,https://github.com/haotian-liu/LLaVA
"Segment Anything",05-04-2023,2304.02643,https://github.com/facebookresearch/segment-anything
"Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection",09-03-2023,2303.05499,https://github.com/IDEA-Research/GroundingDINO
"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",30-01-2023,2301.12597,https://github.com/salesforce/LAVIS
"OWL-ST: Scaling Open-Vocabulary Object Detection",16-01-2023,2306.09683,""
"OWL-ViT: Simple Open-Vocabulary Object Detection with Vision Transformers",12-05-2022,2205.06230,https://github.com/google-research/scenic
"CLIP: Learning Transferable Visual Models From Natural Language Supervision",26-02-2021,2103.00020,https://github.com/openai/CLIP